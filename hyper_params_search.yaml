# Model
# - CNN
# - MLP
# - BinarizedCNN
# - BinarizedMLP
model: "BinarizedCNN"

# Dataset
# 1. MNIST
# 2. CIFAR-10
dataset: "MNIST"

hyperparameters:
  # Common Hyperparameters
  # 1. batch size
  # 2. epochs
  # 3. learning rate

  common:
    batch_size: [1, 256]
    epochs: [1, 100]
    lr_scheduler: False

  # Hyperparameters in BNN
  # 1. stochastic
  # 2. deterministic
  bnn:
    mode: "stochastic"


  # Hyperparameters in Network
  architecture:

    # - CNN
    #   1. number of layer
    #   2. channels
    #   3. kernel size
    #   4. padding
    #   5. stride
    #   6. pooling
    #   7. activation
    #   8. regularization
    #   9. bias
    #   10.batch norm
    #   11.dropout
    cnn:
      num_layer: [1, 10]
      channels: [32]
      kernel_size: 3
      padding: 0
      stride: 1
      pooling: "max_pooling"
      activation: "sigmoid"
      regularization: "l1"
      bias: False
      batch_norm: False
      dropout:
        use: True
        prob: 0.2

    # - MLP
    #   1. number of neurons
    #   2. activation
    #   3. regularization
    #   4. bias
    #   5. batch norm
    #   6. dropout
    mlp:
      num_neuron: 5
      activation: "sigmoid"
      regularization: "l1"
      bias: False
      batch_norm: False
      dropout:
        use: True
        prob: 0.2

  # Optimizer
  # category
  # lr scheduler
  # each optimizer setting
    # SGD
    # Adam
    # AdaMax
    # RMSProp
  optimizer:
    category: "SGD"
    lr_scheduler: False

    SGD:
      learning_rate: 0.01
      momentum: 0.9

    ADAM:
      learning_rate: 0.01
